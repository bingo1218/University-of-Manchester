---
title: "Assignment2"
author: '10878862'
date: "26/04/2022"
output: html_document
---

# Prepare for R markdown

Here, let's turn off the warning and message for later chunks, because during model fitting there are lots of messages about fitting errors, so to make it clear and readable, I set them both as false to ignore this information. It is worth noticing that I have handled all the warnings to make sure all the processing is correct, and I would mention some warning information when necessary. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Classification Problem
Because the classification problem is not quite dissimilar to regression mode, here we omit most of the narrative and just stress the different parts.

## Read in and Tidy Data

```{r}

ClassificationData <- Carseats[Carseats$ShelveLoc=="Good"|Carseats$ShelveLoc=="Bad",]
ClassificationData <- ClassificationData[,c(1,2,6,7,8)]

ClassificationData$ShelveLoc <- droplevels(ClassificationData$ShelveLoc)

ClassificationData$ShelveLoc<- factor(ClassificationData$ShelveLoc, levels = c("Bad", "Good"), labels = c('0', '1'))

str(ClassificationData)
```

## Assumption test 1

### Linearity assumption

```{r}
model <- glm(ShelveLoc ~., data = ClassificationData, 
               family = binomial)
# Select only numeric predictors

probabilities <- predict(model, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "Bad", "Good")

# Bind the logit and tidying the data for plot
mydata <- ClassificationData %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

# Create the scatter plots:
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
The smoothed scatter plots show that these variables are all quite linearly associated with *ShelveLoc* in logit scale, especially for the *Sales*. But the relationship between Age/ComPrice doesn't seem like linear or related. However, the *Price* variables are not 100% perfect, so I think it's fine to try to fit with other methods for those three variables.


### Multicollinearity test

Now, let's check the Multicollinearity problems.

```{r}
model <- glm(ShelveLoc ~., data = ClassificationData, 
               family = binomial)
car::vif(model)
```
The vif of *Sales* and *Price* as well as *ComPrice* >5. We should drop *Price*, which one has highest VIF. Thus, if later we want choose a linear model, we should at least drop several variables. Here, I decided to drop Age and CompPrice because these two variables seems like not associated with outcome.

```{r}
model <- glm(ShelveLoc ~.-CompPrice-Age, data = ClassificationData, 
               family = binomial)
car::vif(model)
```

```{r}
# Examine treme data by the Cookâ€™s distance values.
plot(model, which = 4, id.n = 3)

# Extract model results
model.data <- augment(model) %>% 
  mutate(index = 1:n()) 

model.data %>% top_n(3, .cooksd)
# Plot the standardized residuals:
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = ShelveLoc), alpha = .5) +
  theme_bw()
# Filter potential influential data points with abs(.std.res) > 3:
model.data %>% 
  filter(abs(.std.resid) > 3)
```

As results above, there is some outliers, but no influential observations in our data.

### Fitting *dis*

Again, let's plot the residual value of the model in two different scales.
```{r}
plot(effect("Price", mod = model, partial.residuals=TRUE), type="response")
plot(effect("Price", mod = model, partial.residuals=TRUE), type="link")
```

It seems like there is no non-linear problems for *Price* in logic scales.

#### 2.1.1 Refit *Price* with Polynomial

```{r}
set.seed(1234)
train_control <- trainControl(method="LOOCV")

fit.1 <- train(ShelveLoc ~ Sales + Price,
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.2 <- train(ShelveLoc ~ Sales + poly(Price,2),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.3 <- train(ShelveLoc ~ Sales + poly(Price,3),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.4 <- train(ShelveLoc ~ Sales + poly(Price,4),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.5 <- train(ShelveLoc ~ Sales + poly(Price,5),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.6 <- train(ShelveLoc ~ Sales + poly(Price,6),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.7 <- train(ShelveLoc ~ Sales + poly(Price,7),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
fit.8 <- train(ShelveLoc ~ Sales + poly(Price,8),
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")

Accuracy <- rep(0,8)
Accuracy[1] <- fit.1$results$Accuracy
Accuracy[2] <- fit.2$results$Accuracy
Accuracy[3] <- fit.3$results$Accuracy
Accuracy[4] <- fit.4$results$Accuracy
Accuracy[5] <- fit.5$results$Accuracy
Accuracy[6] <- fit.6$results$Accuracy
Accuracy[7] <- fit.7$results$Accuracy
Accuracy[8] <- fit.8$results$Accuracy

plot(1:8, Accuracy, type = "b", xlab = "degree")

which.max(Accuracy)
Accuracy[which.max(Accuracy)]
```
We choose linear model for *Price* (Accuracy = 0.895).

#### 2.1.2 Refit *Price* with Polynomial Basis Spline

```{r}
Accuracy<- rep(0,18)

for (i in 3:20){
  Data <- ClassificationData
  ClassificationData$Pricebs <- bs(ClassificationData$Price, df = i)
  fit <- train(ShelveLoc ~ Sales + Pricebs,
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
  Accuracy[i-2] <- fit$results$Accuracy
}
plot(3:20, Accuracy, type='b', xlab = 'df')
dfn <- which.max(Accuracy) + 2
dfn
Accuracy[which.max(Accuracy)]
```
We choose basis spline with df = 3 for *Price* (Accuracy = 0.890).


# Nature Spline
```{r}
Accuracy <- rep(0,10)
for (i in 3:13){
  Data <- ClassificationData
  ClassificationData$Pricens <- ns(ClassificationData$Price, df = i)
  fit <- train(ShelveLoc ~ Sales + Pricens,
               method = "glm",
               data = ClassificationData,
               trControl = train_control,
               family = "binomial")
  Accuracy[i-2] <- fit$results$Accuracy
}
plot(3:13, Accuracy, type='b', xlab = 'df')
dfn <- which.max(Accuracy) + 2
dfn
Accuracy[which.max(Accuracy)]

```

Here we choose natural spline with df = 3 (Accuracy = 0.895).

Let's compare linear fit and natural spline fit.
```{r}
model.lm.1 <-glm(ShelveLoc ~ Sales + Price,
               data = ClassificationData,
               family = "binomial")

model.ns.3 <- glm(ShelveLoc ~ Sales + ns(Price,3),
               data = ClassificationData,
               family = "binomial")
plot(effect("Price", model.lm.1, partial.residuals=TRUE), type="response")
plot(effect("Price", model.lm.1, partial.residuals=TRUE), type="link")
plot(effect("Price", model.ns.3, partial.residuals=TRUE), type="response")
plot(effect("Price", model.ns.3, partial.residuals=TRUE), type="link")
```
No significant different, so here, I decided to choose linear fitting for *Price*.


# Best sebset selection
```{r}
## 0 puts
mod.0 <- glm(ShelveLoc ~ 1,
               data = ClassificationData,
               family = "binomial")
## 1 puts
mod.1.1 <- glm(ShelveLoc ~ Sales,
               data = ClassificationData,
               family = "binomial")
mod.1.2 <- glm(ShelveLoc ~ Price,
               data = ClassificationData,
               family = "binomial")
which.max(c(deviance(mod.1.1), deviance(mod.1.2)))
## 2 puts
mod.2.1 <- glm(ShelveLoc ~ Sales + Price,
               data = ClassificationData,
               family = "binomial")
```


```{r}
## Cross-validation
ClassificationData$const <- rep(0,dim(ClassificationData)[1])
fit.0 <- train(ShelveLoc ~ const,
               data = ClassificationData,
               method = "glm",
               trControl = train_control,
               family = "binomial")
fit.1 <- train(ShelveLoc ~ Price,
               data = ClassificationData,
               method = "glm",
               trControl = train_control,
               family = "binomial")
fit.2 <- train(ShelveLoc ~ Sales + Price, 
               data = ClassificationData,
               method = "glm",
               trControl = train_control,
               family = "binomial")

mod.comp <- data.frame(Accuracy = c(fit.0$results$Accuracy,
                               fit.1$results$Accuracy,
                               fit.2$results$Accuracy),
                       Inputs = c(0,1,2))
plot(mod.comp$Inputs, mod.comp$Accuracy, type = "b", xlab = "Number of input variables", ylab = "Accuracy")
dfn_best <- mod.comp$Inputs[which.max(mod.comp$Accuracy)]
dfn_best
```
2-variables is the best.


## Test cllassification model with QDA and KNN
```{r}
  fit.QDA <- train(ShelveLoc ~ Sales + Price, 
                 data = ClassificationData,
                 method = "qda",
                 trControl = train_control)
  fit.KNN <- train(ShelveLoc ~ Sales + Price, 
                 data = ClassificationData,
                 method = "knn",
                 trControl = train_control,
                 preProcess = c("center","scale"))
  
  mod.comp <- data.frame(Accuracy = c(fit.2$results$Accuracy,
                                 fit.QDA$results$Accuracy,
                                 fit.KNN$results$Accuracy[1]),
                         Inputs = c(0,1,2))
                         
  plot(mod.comp$Inputs, mod.comp$Accuracy, type = "b", xlab = "Model", ylab = "Accuracy")
dfn_best <- mod.comp$Inputs[which.max(mod.comp$Accuracy)]
dfn_best

``` 
# Glm 2 variable model is the best.

```{r}
summary(mod.2.1)
```
Here, we could also know 